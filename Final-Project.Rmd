```{r hw2_setup, include=FALSE}
knitr::opts_chunk$set(echo=TRUE, eval=TRUE)
```

# Homework 2 {-}

## 1.
```{r}
# library statements 
# read in data
knitr::opts_chunk$set(echo = TRUE)
library(readr)
library(ggplot2)
library(dplyr)
library(tidymodels)
library(lubridate)
library(fabricatr)
tidymodels_prefer()

accidents <- read_csv("accident_cleanest.csv")
```

```{r}
# how we cleaned the data

#  filter(year(Start_Time) >= 2018, year(Start_Time) <= 2019) %>%
#  drop_na(.) %>%
#  select( -Severity,  -Description, -Number, - Street, -Side, -County, -Zipcode, -Country, -Timezone, -Airport_Code, -Weather_Timestamp, -`Wind_Chill(F)`, -`Humidity(%)`, -`Pressure(in)`, -Wind_Direction, -`Precipitation(in)`, -Amenity, -Bump, -Give_Way, -No_Exit, -Railway, -Roundabout, - Station, -Stop, -Traffic_Calming, -Turning_Loop, -Civil_Twilight, -Nautical_Twilight, -Astronomical_Twilight) %>%
#  sample_frac(size = 1/5) %>%
#  mutate(Crossing = if_else(Crossing, 1, 0)) %>%
#  mutate(Junction = if_else(Junction, 1, 0)) %>%
#  mutate(Traffic_Signal = if_else(Traffic_Signal, 1, 0)) %>%
#  mutate(logDist = log(`Distance(mi)`+.1)) %>%
#  mutate(Duration = round(End_Time - Start_Time)) %>%
#  select(-`Distance(mi)`, - End_Time)
```

```{r}
# creation of cv folds
set.seed(253)

accident_cv <- vfold_cv(accidents, v = 10) # this is the random part

training(accident_cv$splits[[1]]) # pulls training data for the 1st split (1st fold is testing set)
testing(accident_cv$splits[[1]]) # pulls testing data for the 1st split (1st fold is testing set)
```

a. Use ordinary least squares (OLS) by using the `lm` engine and LASSO (`glmnet` engine) to build  a series of initial regression models for your quantitative outcome as a function of the predictors of interest. (As part of data cleaning, exclude any variables that you don't want to consider as predictors.)
  - You'll need two model specifications, `lm_spec` and `lm_lasso_spec` (you'll need to tune this one).
  
```{r}
# model specifications
lm_spec <-
    linear_reg() %>% 
    set_engine(engine = 'lm') %>% 
    set_mode('regression')

lm_lasso_spec <- 
    linear_reg() %>%
    set_args(mixture = 1, penalty = tune()) %>% 
    set_engine(engine = 'glmnet') %>%
    set_mode('regression')

knn_spec <- 
  nearest_neighbor() %>% # new type of model!
  set_args(neighbors = tune()) %>% # tuning parameter is neighbor; tuning spec
  set_engine(engine = 'kknn') %>% # new engine
  set_mode('regression') 
```

b. For each set of variables, you'll need a `recipe` with the `formula`, `data`, and pre-processing steps
  - You may want to have steps in your recipe that remove variables with near zero variance (`step_nzv()`), remove variables that are highly correlated with other variables (`step_corr()`), normalize all quantitative predictors (`step_normalize(all_numeric_predictors())`) and add indicator variables for any categorical variables (`step_dummy(all_nominal_predictors())`).
  - These models should not include any transformations to deal with nonlinearity. You'll explore this in the next investigation.
        
```{r}
# recipes & workflows

car_rec <- recipe(logDist ~ Vis + Temp + Wind + Weather_Condition + Crossing + Junction + Traffic_Signal + Sunrise_Sunset + Duration + dayofweek + Start_Time, data = accidents) %>%
    step_nzv(all_predictors()) %>%
    step_other(all_nominal_predictors()) %>% 
    step_normalize(all_numeric_predictors()) %>%
    step_dummy(all_nominal_predictors()) 

model_wf <- workflow() %>%
    add_recipe(car_rec) %>%
    add_model(lm_spec)

fit_model <- model_wf %>% fit(data = accidents)

car_rec %>% prep(accidents) %>% juice()

lasso_wf_car <- workflow() %>%
    add_recipe(car_rec) %>%
    add_model(lm_lasso_spec)

knn_wf <- workflow() %>%
  add_model(knn_spec) %>%
  add_recipe(car_rec)
```


```{r}
#fit & tune models
mod1_cv <- fit_resamples(model_wf, 
                         resamples = accident_cv, 
                         metrics = metric_set(rmse, rsq, mae))

penalty_grid <- grid_regular(
  penalty(range = c(-3, 1)), #log10 transformed 
  levels = 30)

tune_output <- tune_grid( # new function for tuning parameters
  lasso_wf_car, # workflow
  resamples = accident_cv, # cv folds
  metrics = metric_set(rmse, mae),
  grid = penalty_grid # penalty grid defined above
)

best_se_penalty <- select_by_one_std_err(tune_output, metric = 'mae', desc(penalty))
best_se_penalty # choose penalty value based on the largest penalty within 1 se of the lowest CV MAE

best_penalty <- select_best(tune_output, metric = 'mae')
best_penalty # choose penalty value based on lowest mae

autoplot(tune_output) + theme_classic()

final_wf <- finalize_workflow(lasso_wf_car, best_penalty) # incorporates penalty value to workflow
final_wf_se <- finalize_workflow(lasso_wf_car, best_se_penalty)

final_fit <- fit(final_wf, data = accidents)
final_fit_se <- fit(final_wf_se, data = accidents)

tidy(final_fit)
tidy(final_fit_se) %>% arrange(desc(abs(estimate)))
```
logDist ~ Side + Vis+ Temp+ Wind + Precip+ Weather_Condition + Crossing + Junction + Traffic_Signal + Sunrise_Sunset

### why are the variables in the model different here eg. Side, Vis, Junction missing from tidy? 

c. Estimate the test performance of the models using CV. Report and interpret (with units) the CV metric estimates along with a measure of uncertainty in the estimate (`std_error` is readily available when you used `collect_metrics(summarize=TRUE)`).
  - Compare estimated test performance across the models. Which models(s) might you prefer?

```{r}
#  calculate/collect CV metrics
mod1_cv %>% collect_metrics()
```

d. Use residual plots to evaluate whether some quantitative predictors might be better modeled with nonlinear relationships.

```{r}
# visual residuals

#how do we do this?
mod1_output <- final_fit_se %>% 
    predict(new_data = accidents) %>% #this function maintains the row order of the new_data
    bind_cols(accidents) %>%
    mutate(resid = logDist - .pred)

mod1_output %>% 
  ggplot(aes(x = .pred, y = resid)) + 
  geom_point() +
  geom_smooth() +
  geom_hline(yintercept = 0) +
  theme_classic()

mod1_output %>% 
  ggplot(aes(x = Temp, y = resid)) + 
  geom_point() +
  geom_smooth() +
  geom_hline(yintercept = 0) +
  theme_classic()

mod1_output %>% 
  ggplot(aes(x = Vis, y = resid)) + 
  geom_point() +
  geom_smooth() +
  geom_hline(yintercept = 0) +
  theme_classic()

mod1_output %>% 
  ggplot(aes(x = Wind, y = resid)) + 
  geom_point() +
  geom_smooth() +
  geom_hline(yintercept = 0) +
  theme_classic()

mod1_output %>% 
  ggplot(aes(x = Precip, y = resid)) + 
  geom_point() +
  geom_smooth() +
  geom_hline(yintercept = 0) +
  theme_classic()

mod1_output %>% 
  ggplot(aes(x = dayofweek, y = resid)) + 
  geom_point() +
  geom_smooth() +
  geom_hline(yintercept = 0) +
  theme_classic()

mod1_output %>% 
  ggplot(aes(x = month, y = resid)) + 
  geom_point() +
  geom_smooth() +
  geom_hline(yintercept = 0) +
  theme_classic()
```

e. Which variables do you think are the most important predictors of your quantitative outcome? Justify your answer. Do the methods you've applied reach consensus on which variables are most important? What insights are expected? Surprising?
  - Note that if some (but not all) of the indicator terms for a categorical predictor are selected in the final models, the whole predictor should be treated as selected.



<br>

## 2. 
**Summarize investigations**
    - Decide on an overall best model based on your investigations so far. To do this, make clear your analysis goals. Predictive accuracy? Interpretability? A combination of both?



<br>

## 3. 
**Societal impact**
Are there any harms that may come from your analyses and/or how the data were collected?

One element of the way data was collected that may be harmful is the fact that this information is collected from API’s from Mapquest and Bing which would be tracking live traffic. A lot of the time, this traffic is collected from users of the apps which many times do not realize they have consented to allowing their location information to be collected. As a result, information on traffic duration is made up of individuals who may not want to contribute to the research. Similarly, another harm of the dataset could be the possible inclusion of multiples when recording the accidents. The data collectors took measures to filter out any duplicates, but there is a possibility that there are still some left in that could affect the integrity of our results. 

What cautions do you want to keep in mind when communicating your work?

We want to make note that since the dataset is so extensive ( >1M cases), we needed to filter the dataset down to almost half of the original amount in order to be able to run our models. As a result, we are only looking at specific years, which may be reflective of whatever the conditions of overall traffic patterns are. For example, by excluding 2020, we are not measuring the effect COVID-19 had on traffic. Likewise, not incorporating 2016 and 2017 will not take into account the lower gas prices that may have led to higher traffic volumes. As a result, we need to take these and other possible shortcomings of the dataset into consideration.

# Homework 3 {-}

## 2.
**Accounting for nonlinearity**
a. Update your OLS model(s) and LASSO model to use natural splines for the quantitative predictors.
  -You’ll need to update the recipe to include step_ns() for each quantitative predictor.
  -It’s recommended to use few knots (e.g., 2 knots = 3 degrees of freedom).
  
```{r}
gam_spec <- 
  gen_additive_mod() %>%
  set_engine(engine = 'mgcv') %>%
  set_mode('regression') 

gam_mod <- fit(gam_spec,
               logDist ~ Side + s(Vis) + s(Temp,k=11) + Wind + Precip + Weather_Condition + Crossing + Junction + Traffic_Signal + Sunrise_Sunset, 
               data = accidents)

par(mfrow=c(2,2))
gam_mod %>% pluck('fit') %>% mgcv::gam.check()
gam_mod %>% pluck('fit') %>% summary() 
gam_mod %>% pluck('fit') %>%plot( page = 1)
```
  
```{r}
spline_rec <- recipe(logDist ~ ., data = accidents) %>% 
  step_rm(ID) %>%
  #step_rm(`Weather_Condition_Partly.Cloudy`) %>%
  step_other(all_nominal_predictors()) %>%
  step_dummy(all_nominal_predictors())
  
  #step_ns(Terminal, deg_free = 3) %>%
  #step_ns(Expend, deg_free = 3) #ask Brianna about what we need

# Check the pre-processed data
spline_rec %>% prep(accidents) %>% juice() #why did ID show up? How to analyze? How do we know which ones to do stepns for?
# How do we know which predictors are the most relevant?
```

```{r}
lm_spec <-
  linear_reg() %>%
  set_engine(engine = 'lm') %>%
  set_mode('regression')

# Workflow (Recipe + Model)
spline_wf <- workflow() %>% 
  add_recipe(spline_rec) %>%
  add_model(lm_spec) 


# CV to Evaluate
cv_output <- fit_resamples( 
  spline_wf, # workflow
  resamples =accident_cv, # cv folds
  metrics = metric_set(mae)
)
cv_output %>% collect_metrics()

# Fit model
ns_mod <- spline_wf %>%
  fit(data = accidents) 

ns_mod %>%
  tidy() %>%
  arrange(desc(p.value))

```


b. Compare insights from variable importance analyses here and the corresponding results from the Investigation 1. Now after having accounted for nonlinearity, have the most relevant predictors changed? 
  -Note that if some (but not all) of the spline terms are selected in the final models, the whole predictor should be treated as selected.
  
### we didn't get past this... we have to redo some of the part 1 code as we standardized some variables; but have all reached our capacity for this assignment and want to enjoy break.

c. Fit a GAM using spline terms using the set of variables deemed to be most relevant based on your investigations so far.
  -How does test performance of the GAM compare to other models you explored?
  -Do you gain any insights from the GAM output plots for each predictor?


3. Summarize investigations
  -Decide on an overall best model based on your investigations so far. To do this, make clear your analysis goals. Predictive accuracy? Interpretability? A combination of both?


4. Societal impact
  -Are there any harms that may come from your analyses and/or how the data were collected?
  -What cautions do you want to keep in mind when communicating your work?

# Homework 4 {-}
Specify the research question for a classification task.

**What factors best predict whether an accident's impact on traffic is low, medium or high?**

Try to implement at least 2 different classification methods to answer your research question.

```{r, warning=FALSE}
# splitting distance into low, medium and high
# these will be two different log graphs—low to med and med to high

accidents %>%
  mutate(distRank = ntile(accidents$logDist, 3)) %>%
  mutate(distRank = relevel(factor(distRank), ref = 1)) %>%
  filter(distRank %in% c(1,2)) -> lowMedDist  #compares low to medium distance
         
accidents %>%
  mutate(distRank = ntile(accidents$logDist, 3)) %>%
  mutate(distRank = relevel(factor(distRank), ref = 2)) %>%
  filter(distRank %in% c(2,3)) -> medHighDist #compares medium to high distance

# CV Folds for each
lowMed_cv10 <- vfold_cv(lowMedDist, v = 10)
medHigh_cv10 <- vfold_cv(medHighDist, v = 10)

# Tuning Specification
logistic_lasso_spec_tune <- logistic_reg() %>%
    set_engine('glmnet') %>%
    set_args(mixture = 1, penalty = tune()) %>%
    set_mode('classification')
```

```{r}
# for lowMed subset:

# Recipe 
logistic_rec <- recipe(distRank ~ Start_Time, Duration, dayofweek, Temp,
                       Vis, Wind, Weather_Condition, Crossing, Junction,
                       Traffic_Signal, Sunrise_Sunset,
                       data = lowMedDist) %>%
    step_normalize(all_numeric_predictors()) %>%
    step_other(all_nominal_predictors()) %>%
    step_dummy(all_nominal_predictors())

# Workflow (Recipe + Model)
log_lasso_wf <- workflow() %>%
    add_recipe(logistic_rec) %>%
    add_model(logistic_lasso_spec_tune)

# Tune Model
penalty_grid <- grid_regular(
  penalty(range = c(-3, 1)), #log10 transformed
  levels = 30)

# Tune Output
tune_output <- tune_grid(
  log_lasso_wf, # workflow
  resamples = lowMed_cv10, # cv folds
  metrics = metric_set(roc_auc),
  control = control_resamples(save_pred = TRUE, event_level = 'second'),
  grid = penalty_grid # penalty grid defined above
)

# Tune Output results in an error, unsure why but we predict it is a problem with the formatting of our categorical variables
```



```{r}
penalty_grid <- grid_regular(
  neighbors(range = c(1, 100)), #  min and max of values for neighbors
  levels = 20) # number of neighbors values
knn_fit_cv <- tune_grid(knn_wf, # workflow
              resamples = accident_cv, #CV folds
              grid = penalty_grid, # grid specified above
              metrics = metric_set(rmse, mae))

knn_fit_cv %>% autoplot() + theme_classic()
knn_fit_cv %>% show_best(metric = 'mae') 

tuned_knn_wf <- knn_fit_cv %>% 
  select_by_one_std_err(metric = 'mae', desc(neighbors)) %>%  # Choose neighbors value that leads to the highest neighbors within 1 se of the lowest CV MAE
  finalize_workflow(knn_wf, .)

# Fit final KNN model to data
knn_fit_final <- tuned_knn_wf %>%
  fit(data = accidents) 

#--------------------

knn_mod_out <- knn_fit_final %>%
    predict(new_data = accidents) %>%
    bind_cols(accidents) %>%
    mutate(resid = logDist - .pred)

knn_mod_out %>% 
  ggplot(aes(x = .pred, y = resid)) + 
  geom_point() +
  geom_smooth(se = FALSE) + 
  geom_hline(yintercept = 0) + 
  theme_classic()
```


Reflect on the information gained from these two methods and how you might justify this method to others.

Keep in mind that the final project will require you to complete the pieces below. Use this as a guide for your work but don’t try to accomplish everything for HW4:

Classification - Methods
Indicate at least 2 different methods used to answer your classification research question.
Describe what you did to evaluate the models explored.
Indicate how you estimated quantitative evaluation metrics.
Describe the goals / purpose of the methods used in the overall context of your research investigations.

Classification - Results
Summarize your final model and justify your model choice (see below for ways to justify your choice).
Compare the different classification models tried in light of evaluation metrics, variable importance, and data context.
Display evaluation metrics for different models in a clean, organized way. This display should include both the estimated metric as well as its standard deviation. (This won’t be available from OOB error estimation. If using OOB, don’t worry about reporting the SD.)
Broadly summarize conclusions from looking at these evaluation metrics and their measures of uncertainty.

Classification - Conclusions - 
Interpret evaluation metric(s) for the final model in context. Does the model show an acceptable amount of error? - If using OOB error estimation, display the test (OOB) confusion matrix, and use it to interpret the strengths and weaknesses of the final model. - Summarization should show evidence of acknowledging the data context in thinking about the sensibility of these results.



















